---
title: 词向量
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - AI
  - NLP
tags:
  - AI
  - NLP
keywords: 词向量
abbrlink: 词向量
date: 2023-07-18 09:50:52
---

NLP 绕不开的第一个概念就是词向量。

<!--more-->

# 语言算法表达

计算机看文字就跟我看乱码一样，看不懂也没法直接处理。所以 NLP 第一步：将文字转换为计算机看得懂的编码。一般的程序会将字符串转为特殊字符的编码，但在深度学习领域，必须将文本转换为数字。

## 字典序

最简单的当然是字典序：该词为词典中的第几个，比如 Abandon 编码 为 1。

![](https://emojidaquan.com/Pics/meme/202302/3ede44f39bf2.jpg)

字典序本质为间隔为 1 的离散变量。离散变量并非无法训练，有些模型亦可使用离散变量进行训练。但字典序除了体现单词的字典顺序，P 用没有。

## 独热编码 one-hot

构建一个 vocab\_size<sub>字典大小</sub> 个 $0$ 组成的序列，将字典序数字的 $0$ 换为 $1$

比如 $n$ 为 5，abandon 字典序为 2，则其对应 one-hot 编码为

> 0，1，0，0，0

**优点：**

- 数值上每个字典序转换后有且只有一个 1，避免字典序的数值问题
- 作为连续变量训练时，0~1可以代表出现概率，赋予了意义

# 词向量 embedding

为嘛不直接用 one-hot 呢？

- one-hot 编码字典有多少字就要多少维向量，很容易计算爆炸
- one-hot 编码只是编码，没有其他的语义特征，仅仅是方便了处理。但我们在使用语言时是有好恶，褒义词、贬义词的。

如何将语义融入到词表示中？Harris提出的**分布假说**为这一设想提供了理论基础：**上下文相似的词，其语义也相似。**而基于基于分布假说的词表示方法，根据建模的不同，主要可以分为三类：基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。word embedding一般来说就是一种基于神经网络的分布表示。

向量空间将词汇嵌套在一个连续向量空间中，语义近似的词汇被映射为相邻的数据点。采用这一假设的研究方法大致分为以下两类：基于计数的方法 (e.g. 潜在语义分析， Glove)， 和 预测方法 (e.g. 神经概率化语言模型，word2vec)

## Word2vec



## Embedding Layer



# 参考文献

[【白话NLP】什么是词向量](https://zhuanlan.zhihu.com/p/81032021)