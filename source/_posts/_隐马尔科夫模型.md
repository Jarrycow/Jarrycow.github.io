---
title: 隐马尔科夫模型
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - AI
tags:
  - AI
  - 机器学习
keywords: 隐马尔科夫模型
abbrlink: 隐马尔科夫模型
date: 2023-07-27 00:56:14
---

隐马尔科夫模型 HMM 是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。

<!--more-->

# 隐马尔可夫模型HMM

## 什么问题要 HMM 亲自来做

使用HMM模型时我们的问题一般有这两个特征：

1. 问题是基于序列的，比如时间序列，或者状态序列。
2. 问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。

## 什么是 HMM

HMM 模型是有一个会随时间改变的不可见的状态，该状态持续影响外在表现的系统。

- 状态和状态间转换的概率
- 不同状态下，有着不同的外在表现的概率。
- 最开始设置的初始状态
- 能转换的所有状态的集合
- 能观察到外在表现的结合

**定义**：隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。

- **状态序列**：隐藏的马尔可夫链随机生成的状态的序列

- **观测序列**：每个状态生成一个观测，而由此产生的观测的随机序列

假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：
$$
Q=\left\{ q_1,q_2,\cdots ,q_N \right\} \quad V=\left\{ v_1,v_2,\cdots \text{v}_M \right\}
$$
对于长度 T 的序列，I 对应状态序列，O 对应观察序列
$$
I=\left\{ i_1,i_2,\cdots ,i_T \right\} \quad O=\left\{ o_1,o_2,\cdots o_M \right\}
$$
其中 $\forall i_t\in Q$，$\forall o_t\in V$

**HMM 假设：**

- 齐次马尔科夫链假设：任意时刻的隐藏状态只依赖于它前一个隐藏状态

  若 $t$ 时刻隐藏状态 $i_t=q_i$，$t+1$ 时刻隐藏状态 $i_{t+1}=q_j$，则 HMM 状态转移概率 $a_{ij}$：
  $$
  a_{ij}=P\left(i_{t+1}=q_j\left|\right.i_t=q_i\right)
  $$
  $a_{ij}$ 即可组成马尔科夫链状态转移方程 $A$：
  $$
  A=\left[a_{ij}\right]_{N\times N}
  $$

- 观测独立性假设：任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态

  若 $t$ 时刻隐藏状态 $i_t=q_j$，对应观察状态 $o_t=v_k$，则该时刻观察状态 $v_k$ 在隐藏状态 $q_j$ 下生成的概率 $b_j(k)$ 满足：
  $$
  b_j\left(k\right)=P\left(o_{t}=v_k\left|\right.i_t=q_i\right)
  $$
  则 $b_j(k)$ 可以组成观测状态生成的概率矩阵 $B$：
  $$
  B=\left[b_j\left(k\right)\right]_{N\times M}
  $$

除此之外，还需要一组 $t=1$ 时刻隐藏状态概率分布 $\varPi$：
$$
\varPi=\left[\pi(i)\right]_N
$$
其中，$\pi(i)=P\left(i_1=q_i\right)$

HMM 模型可由隐藏状态概率分布 $\varPi$、状态转移概率方程 $A$、观测状态概率矩阵 $B$ 决定。
$$
\lambda=\left(A,B,H\right)
$$

![HMM](https://raw.githubusercontent.com/Jarrycow/picHost/main/JavaWeb/v2-d4077c2dbd9899d8896751a28490c9c7_720w.webp)

# HMM 解决问题

## 观测序列生成算法

**输入：**HMM 模型 $\lambda=\left(A,B,\pi\right)$

**输出：**观测序列 $O=\left\{ o_1,o_2,\cdots ,o_T \right\}$

**生成过程：**

1. 按照初始状态分布 $\pi$ 产生状态 $i_1$

2. 令 $T = 1$

3. 按照状态 $i_t$ 的观测概率分布 $b_{i_t}(k)$ 生成 $o_t$

4. 按照状 $i_t$ 的状态转移概率分布 ${ α_{i, i_(t+1)} }$ 产生状态 $i_{t+1}$

5. 令 $t = t + 1$；

   如果 $t < T$，转步 3；

   否则，终止

## 评估观察序列概率

**已知：** 

- HMM：$\lambda=\left(A,B,\pi\right)$
- 观测序列 $O=\left\{ o_1,o_2,\cdots ,o_T \right\}$

**求：**观测序列 $O$ 在模型 $\lambda$ 下条件概率 $P\left(O\left|\right.\lambda\right)$

显然可以直接暴力求解，但是复杂度较高。因此采用**前向后向算法**进行求解

### 前向算法求HMM观测序列的概率

前向算法本质上属于动态规划的算法，也就是我们要通过找到局部状态递推的公式，这样一步步的从子问题的最优解拓展到整个问题的最优解。

1. 计算时刻 1 各个隐藏状态的前向概率
   $$
   a_1\left(i\right)=\pi_ib_i\left(o_1\right) \quad i=1,2,\cdots,N
   $$

2. 递推后面时刻的前向概率
   $$
   a_{t+1}\left(i\right)=\left[\sum^N_{j=1}a_t(j)a_{ji}\right]b_i\left(o_{t+1}\right)\quad i =1,2,\cdots,N
   $$

3. 计算最终结果
   $$
   P\left(O\left|\right.\lambda\right)=\sum_{i=1}^N\alpha_T(i)
   $$
   

**时间复杂度：**$O\left(TN^2\right)$

### 后向算法求HMM观测序列的概率

后向算法和前向算法非常类似，都是用的动态规划，唯一的区别是选择的局部状态不同，后向算法用的是“后向概率

1. 初始化 $T$ 时刻各个隐藏状态各个隐藏状态后向概率：
   $$
   \beta_T(i)=1\quad i=1,2,\cdots,N
   $$

2. 递推时刻 $T-1,T-2,\cdots,1$ 时刻后向概率：
   $$
   \beta_t(i)=\sum_{j=1}^N a_{ij}b_j\left(o_{t+1}\right)\beta_{t+1}(j)\quad i=1,2,\cdots,N
   $$

3. 计算最终结果
   $$
   P\left(O\left|\right.\lambda\right)=\sum_{i=1}^N\pi_ib_i\left(o_1\right)\beta_1\left(i\right)
   $$

**时间复杂度：**$O\left(TN^2\right)$

### HMM 常用概率

- 给定模型 $\lambda$ 和观测序列 $O$ 在时刻 $t$ 处于状态 $q_i$ 概率：
  $$
  \gamma_t(i)=\dfrac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}
  $$

- 给定模型 $\lambda$ 和观测序列 $O$ 在时刻 $t$ 处于状态 $q_i$ 且时刻 $t+1$ 处于状态 $q_j$ 概率：
  $$
  \xi_t(i,j)=\dfrac{\alpha_t(i)a_{ij}b_j\left(o_{t+1}\right)\beta_{t+1}(j)}{\sum_{r=1}^N\sum_{s=1}^N\alpha_t(r)a_{rs}b_s\left(o_{t+1}\right)\beta_{t+1}\left(s\right)}
  $$

- 在观测序列 $O$ 下状态 $i$ 出现的期望值 $\sum_{t=1}^T\gamma_t(i)$

- 在观测序列 $O$ 下状态 $i$ 转移的期望值 $\sum_{t=1}^{T-1}\gamma_t(i)$

- 在观测序列 $O$ 下状态 $i$ 转移到状态 $j$ 的期望值 $\sum_{t=1}^{T-1}\xi_t(i,j)$

## 模型参数学习问题

**输入：**$D$ 个观测序列样本 $\left\{ \left( O_1 \right) ,\left( O_2 \right) ,\cdots \left( O_D \right) \right\} $

**输出：**HMM 模型参数

### 鲍姆-韦尔奇算法求解HMM参数

1. 随机初始化 $\pi_i$、$a_{ij}$、$b_j(k)$

2. 对于每个样本 $d=1,2,\cdots,D$，用前向后向算法计算 $\gamma_t^\left(d\right)(i)$、$\xi_t^{(d)}(i,j)$$\quad t=1,2,\cdots,T$

3. 更新模型参数
   $$
   \pi_i=\dfrac{\sum_{d=1}^D\gamma_1^{(d)}(i)}{D}
   \\
   a_{ij}=\dfrac{\sum_{d=1}^D\sum_{t=1}^{T-1}\xi_t^{(d)(i,j)}}{\sum_{d=1}^D\sum_{t=1}^{T-1}\gamma_t^{(d)}(i)}
   \\
   b_j(k)=\dfrac{\sum_{d=1}^D\sum^T_{t=1,o_t^{(d)}=v_k}\gamma_t^{(d)}(i)}{\sum_{d=1}^D\sum_{t=1}^T\gamma_t^{(d)}(i)}
   $$
   
4. 若 $\pi_i$、$a_{ij}$、$b_j(k)$ 收敛，结束算法；否则返回 2

## 预测问题

给定模型和观测序列，求给定观测序列条件下，最可能出现的对应的隐藏状态序列

**输入：**

- HMM：$\lambda=\left(A,B,\pi\right)$
- 观测序列 $O=\left\{ o_1,o_2,\cdots ,o_T \right\}$

**输出：**最有可能隐藏状态序列 $I^*=\left\{ i_{1}^{*},i_{2}^{*},\cdots ,i_{T}^{*} \right\}$

### 维特比算法解码隐藏状态序列

维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法。

1. 初始化局部状态
   $$
   \delta _1\left( i \right) =\pi _ib_i\left( o_1 \right) \quad i=1,2,\cdots ,N
   \\
   \psi _1\left( i \right) =0\quad i=1,2,\cdots ,N
   $$

2. 进行动态规划递推后面时刻 $t$ 局部状态：
   $$
   \delta _t\left( i \right) =\underset{1\le j\le N}{\max}\left[ \delta _{t-1}\left( j \right) a_{ji} \right] b_i\left( 0_t \right) \quad i=1,2,\cdots ,N
   \\
   \psi _t\left( i \right) =\text{arg\ }\underset{1\le j\le N}{\max}\left[ \delta _{t-1}\left( j \right) a_{ji} \right] \quad i=1,2,\cdots ,N
   $$

3. 计算时刻 $T$ 最大 $\delta_T(i)$ 即为最有可能隐藏状态序列出现的概率；最大的 $\psi_t(i)$ 即为最可能隐藏状态
   $$
   P*=\underset{1\le j\le N}{\max}\delta_T(i)
   \\
   i^*_T=\text{arg}\underset{1\le j\le N}{\max}\left[\delta_T(i)\right]
   $$

4. 利用局部状态回溯
   $$
   i^*_t=\psi_{t+1}\left(i_{t+1}^*\right)
   $$
   最终得到可能隐藏状态序列 $I^*=\left\{ i_{1}^{*},i_{2}^{*},\cdots ,i_{T}^{*} \right\} $

# 参考文献

[隐马尔可夫模型HMM](https://zhuanlan.zhihu.com/p/29938926)

[HMM隐马尔科夫模型](https://luweikxy.gitbook.io/machine-learning-notes/hidden-markov-model#wei-te-bi-suan-fa)