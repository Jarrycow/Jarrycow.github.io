---
title: 面试
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - null
tags:
  - null
keywords: 面试
abbrlink: 面试
date: 2023-08-30 15:01:39
---

[toc]



# 自我介绍

面试官您好，我叫孔健睿，来自东南大学网络空间安全学院，现在是一名研二的硕士研究生，我的导师是翟玉庆教授。很荣幸能够有机会参加本次面试。

在校期间，我参与“基于计算机视觉交通智能应用”项目，使用YoloV5实现交通智能检测，并获得软件杯国赛二等奖。我还参加“基于CNN-BiGRU”成绩预测方法大创项目。该项目已实际投入应用并发表在审专利。

进入硕士阶段，我主修网络测量、模式识别、人工智能和机器学习等课程。同时因课题组需要，我开始学习NLP相关知识，并在导师的课程中完成“基于知识图谱的东南大学导师信息智能问答系统”。

除此之外，我在校期间参加多场竞赛，获得研究生数学建模国赛二等奖、美赛H奖等奖项。我还积极参与各种学生工作，获得东南大学优秀党员等荣誉。

我希望通过这次实习机会，加强我在NLP方面的学习，也能够将我的研究方向和企业实际需求相结合，为我的科研工作拓宽思路。我的自我介绍到此结束，谢谢面试官的聆听。

## 去年一年有哪些项目

去年我的导师没有给我们安排项目，主要任务还是让我们进行课程的学习。去年我选修的课程有网络测量、模式识别、人工智能和机器学习。这些课程导师会让我们做一些项目来检验学习成果。比如网络测量课程上，我复现了去年一篇IEEE论文并略加修改，使用决策树和BiGUR两阶段对加密流量实现分类。再比如人工智能和机器学习课程，也就是我导师的课程，我完成了基于知识图谱的东南大学导师信息智能问答系统。

## 反问

- 请问这个岗位进去具体是做什么的

# 知识图谱

## 项目描述

首先，我爬取了每个学院官网学校所有导师的信息。之后，将这些信息用 BiGRU-CRF 进行实体识别，用 bert 进行关系抽取，并在 Neo4j 图数据库进行知识存储。对于用户提问，进行意图识别和槽位填充，构建智能问答系统。

## BiGRU-CRF

该模型使用双向 LSTM 网络提取特征，然后在输出层接一个 CRF 层做解码，目的就是上文所说的找预测标签的最优输出路径

## Bert

该模型没有使用上述模型中的位置向量，而是通过在实体前后加标识符的方式表明实体位置。

1. 开头之前添加 CLS
2. 第一个实体前后添加$
3. 第二个实体之间添加#

两个实体前后添加特殊符号的目的是标识两个实体，让模型能够知道这两个词的特殊性,相当于变相指出两个实体的位置。

# 智能交通

## YOLOv5

YoloV5的Head侧在YOLOv4的基础上**引入了自适应anchor box和邻域正负样本分配策略**。

比起yolov4中一个ground truth只能匹配一个正样本，YOLOv5能够在多个grid cell中都分配到正样本，有助于训练加速和正负样本平衡。

## DeepSort

多目标追踪算法：核心思想是将目标检测和目标跟踪分离开来，首先使用目标检测算法对视频帧中的目标进行检测，然后将检测到的目标转化为特征向量，使用深度学习模型对目标特征进行学习， 最后使用==卡尔曼滤波器==对目标进行跟踪，实现对目标的连续跟踪。 

**卡尔曼滤波算法**：根据你检测框的位置去预测目标在下一帧的位置，他是线性的

**匈牙利算法：**通过不断寻找增广路径的办法，寻找最大匹配数量。这就是匈牙利算法的主要思路

对于车辆跟踪来说，就是将**检测框**与**跟踪框（卡尔曼滤波预测出来的跟踪框）\**他们匹配起来，找到最优（最佳）的匹配，从而实现\**跟踪**。

## MTCNN/LPRNet

**MTCNN**

MTCNN网络采用三网级联结构，首先将图像重新缩放为不同大小的范围（称为图像金字塔

在第一阶段，它通过浅层 CNN 快速生成候选窗口。

然后，它通过更复杂的 CNN 对窗口进行细化以拒绝大量非人脸窗口。

模型被称为多任务网络，因为级联中的三个模型（P-Net、R-Net 和 O-Net）中的每一个都在三个任务上进行训练，例如进行三种类型的预测：人脸分类、边界框回归和人脸 landmark 定位。三个模型不直接连接，相反，前一阶段的输出作为输入送到下一阶段。这允许在阶段之间执行额外的处理

**LPRNET**

LPRNet是一种非常高效的神经网络，是第一个没使用RNN的实时轻量级算法。LPRNet由轻量级的卷积神经网络组成，所以它可以采用端到端的方法来进行训练。

本质是个分类网络，定义STN网络结构

STN提出的空间网络变换层，具有平移不变性、旋转不变性及缩放不变性等强大的性能。这个网络可以加在现有的卷积网络中，提高分类的准确性。

**CCPD数据集**

CCPD(中国城市停车数据集，ECCV)和PDRC(车牌检测与识别挑战)。这是一个用于车牌识别的大型国内的数据集，由中科大的科研人员构建出来的。

# CNN-BiGRU

## 简单介绍下CNN

1. 卷积层：卷积操作是CNN的核心。卷积层使用一系列小的卷积核来在输入图像上滑动，执行元素级别的乘法和加法操作。用于检测图像中的不同特征，如边缘、纹理和形状。
2. 池化层：池化层用于减小特征图的空间维度，从而降低计算复杂性并增加模型的鲁棒性。最常见的池化操作是最大池化。
3. 激活函数：在卷积层之后通常会添加激活函数，常见的激活函数包括ReLU和Sigmoid。这些函数引入非线性性，使网络能够学习复杂的特征。
4. 全连接层（Fully Connected Layer）：在池化层之后，通常会添加一个或多个全连接层，用于将前面层的特征映射转换成一个适合用于分类或回归的向量。全连接层包含权重和偏差，用于学习特定任务的模式。

## LSTM

LSTM（长短期记忆网络）是一种时间递归神经网络，适合处理和预测时间序列中间隔和延迟相对较长的重要事件。LSTM的核心概念在于细胞状态以及“门”结构，其中细胞状态相当于信息传输的路径，让信息能在序列连中传递下去。而“门”结构则用于添加或移除信息，通过训练过程学习该保存或遗忘哪些信息。

### LSTM三个门

**输入门 Input Gate：**当外界的输出值想要写入到LSTM里面的时候，必须先通过一个闸门 Input Gate，并且只有当 Input Gate 打开的时候，才能把值写入到 memory 中去；当 Input Gate 关起来的时候，就无法写入值

**输出门 Output Gate：**输出的地方有一个 Output Gate，可以决定外界是否可以把值从 memory 里面读出。只有当 Output Gate 打开的时候，外界才能读取。

**遗忘门 Forget Gate：**Forget Gate 决定什么时候把过去记得的东西（存在memory中的值）忘掉，什么时候记住过去学的东西。

### LSTM流程

**输入：**

- 输入 $z$ 
- 通过激活函数得到 $g(z)$
- 输入门控制信号 $z_i$经过激活函数得到 $f\left(z_i\right)$
- 两者相乘得到 $g(z)f\left(z_i\right)$

**Memory 更新：**

- Memory 中原有值 $c$

- 遗忘门控制信号 $z_f$ 经过激活函数得到 $f\left(z_f\right)$

  $f\left(z_f\right)$ 为 1 代表记忆；$f\left(z_f\right)$ 为 0 代表遗忘

- 加上输入值得到新的 Memory 中要存储的值

  $c'=g(z)f\left(z_i\right)+cf\left(z_f\right)$

**输出：**

- Memory 中值 $c'$ 经过激活函数得 $h\left(c'\right)$
- 输出门的控制信号 $z_0$ 经过激活函数得到的 $f\left(z_0\right)$

- 两者相乘得到输出值 $a=h\left(c'\right)f\left(z_0\right)$

**公式：**

- 遗忘门：$f_t=\sigma \left( W_f\cdot \left[ h_{t-1},x_t \right] +b_f \right) $

- 更新门：

  $i_t=\sigma \left( W_i\cdot \left[ h_{t-1},x_t \right] +b_i \right) $

  $\tilde{C}_t=\tan\text{h}\left( W_C\cdot \left[ h_{t-1},x_t \right] +b_c \right) $

  $C_t=f_t\cdot C_{t-1}+i_t\cdot \tilde{C}_t$

- 输出门：

  $o_t=\sigma \left( W_o\cdot \left[ h_{t-1},x_t \right] +b_O \right) $

  $h_t=o_t\cdot \tanh C_t$

## GRU

**公式：**

- 重置门：

  $r_t=\sigma \left( W_r\cdot \left[ h_{t-1},x_t \right] +b_r \right) $

  $\tilde{h}_t=\tan\text{h}\left( W\cdot \left[ r_t\cdot h_{t-1},x_t \right] +b_c \right) $

- 更新门：

  $h_t=\left( 1-z_t \right) \cdot h_{t-1}+z_t\cdot \tilde{h}_t$

  $z_t=\sigma \left( W_z\cdot \left[ h_{t-1},x_t \right] +b_z \right) $

一般来说两者效果差不多，性能在很多任务上也不分伯仲。GRU参数更少，收敛更快；数据量很大时，LSTM效果会更好一些，因为LSTM参数也比GRU参数多一些。

## 优化器

### 优化器的作用

更新和计算影响模型训练和模型输出的网络参数，使其逼近或达到最优值，从而最小化（或最大化）损失函数。

### 各种优化器如何选择

1. 刚入门选NAG或Adam
2. 如果**模型非常稀疏**，优先考虑**自适应学习率**的算法
3. 随机梯度下降算法的收敛速度和数据集大小的关系不大。因此，可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，然后通过参数搜索的方式寻找最优的训练参数。

### Adam相较于传统的SGD的优点是什么？

Adam结合SGDM和AdaDelta两种优化算法的优点。对梯度的一阶动量（惯性）和二阶动量（更新频率）进行综合考虑，计算出更新步长。**一阶动量**的优势在于他能够学习到历史梯度下降的惯性，避免受到单个样本分布的干扰，**减少震荡**，加快收敛；**二阶动量**的优势在于是**自适应学习率**，为参数的不同维分配不同的学习率，在**模型稀疏的情况下效果很好**。

## 梯度消失和梯度爆炸

防止梯度爆炸：

1. 梯度剪切：更新梯度时，梯度超过某个阈值，就将其强制限制在这个范围内
2. 权重正则化：L1正则和L2正则

防止梯度消失：

1. 合理的激活函数（如ReLU）+权重初始化
2. Batch Normalization：应用于每层激活函数之前
3. 残差网络

## LN&BN

LN（Layer Normalization）和BN（Batch Normalization）都是用于神经网络中的正则化技术，它们的主要目的是加速神经网络的训练并提高其收敛速度。

- 计算方式
  - BN是在每个网络层的输入上进行归一化操作，具体来说，对于每个输入批次（一组样本）归一化
  - LN是在每个神经网络层的每个神经元的输出上进行归一化操作。对于每个神经元，LN计算归一化
- 对训练样本数量的依赖
  - BN依赖每个小批次样本数据进行计算，因此在训练过程中会受到小批次样本的影响。LN不受影响
- 在不同网络结构中的应用
  - BN常用于CNN、DNN，特别是在较深的网络中，以减轻梯度消失和爆炸问题，加速训练。
  - LN通常用于递归神经网络（RNN）和一些特定的神经网络结构，因为它对序列数据和不同长度的输入更具鲁棒性。

### 文本中用LN更好，而在图片中要跨batch做归一化

在文本中使用BN会遇到以下问题：

1. 各个样本的长度都是不同的，但统计到比较靠后的时间片时，可能只有一个样本还有特征数据，这时基于这个样本的统计信息无法反映全局分布，这时的BN效果会不好。
2. 如果在测试时遇到长度大于任何一个训练样本的测试样本，无法找到保存的归一化统计量，所以BN无法运行。
3. 以上问题，都是由于计算归一化统计量时计算的样本数太少。 LN独立于batch-size，无论样本数多少都不会影响到参与LN计算的数据量。

## 提升模型指标的调参技巧

- 先在一个较小的训练集上train和test，看看会不会过拟合。
- 看train/eval的loss曲线，正常的情况应该是train loss呈log状一直下降最后趋于稳定，eval loss开始时一直下降到某一个epoch之后开始趋于稳定或开始上升，这时候可以用early stopping保存eval loss最低的那个模型。
- 权重平均，把这几个局部最优解拿过来，做一个均值操作，再让网络加载这个权重进行预测
- 初始参数以正态分布随机初始化，且保持神经元的输入和输出的方差一致，避免了所有输出值趋向于0的情况
- 模型蒸馏

1. 在训练过程中，将教师模型的输出（logits）和学生模型的输出（logits）都传入softmax函数，计算出对应的概率分布。
2. 计算两个概率分布之间的Kullback-Leibler (KL)散度或JS散度，作为损失函数。这个损失函数代表了教师模型和学生模型对每个数据点的预测分布之间的差异。
3. 在训练过程中，优化这个损失函数，使得学生模型的输出逐渐接近教师模型的输出。

## 过拟合的解决办法有哪些？

1. 交叉验证，通过交叉验证得到较优的模型参数

2. 特征选择，减少特征数或者使用较少的特征组合，对于按区间离散化的特征，增大划分区间。（当样本特征数很多，而样本数很少的时候，很容易陷入过拟合。可以以多元方程式为例，样本少时，如果方程式中的参数越多，越容易过拟合。）

3. 正则化，常用的有L1、L2正则。而且L1正则化可以自动进行特征选择；如果有正则项可以考虑增大正则项参数lambda。

   通过引入权重参数来限制模型复杂度，从而提高模型的泛化能力。正则化可理解为是一种“罚函数法”，即对不希望得到的结果施以惩罚，从而使得优化过程趋向于希望目标。

4. 增加训练数据可以有效的避免过拟合。减少了噪声的影响。

5. bagging，将多个弱分类器bagging一下效果会更好，比如随机森林等。

6. 降低模型复杂度。

7. 提前终止：在测试误差开始上升之前，就停止训练,即使此时训练尚未收敛(即训练误差未达到最小值)。

# NLP 技术

## jieba 分词原理

jieba是一个开源的中文文本分词工具，用于将中文文本切分为词语，在NLP中广泛使用。jieba主要基于一种称为“基于字典的最大概率分词”的方法，同时结合隐马尔可夫模型处理未登录词。

首先，jieba加载一个包含常见中文词汇和词频的词典。进行分词时，对待句子进行扫码，生成有向无环图。接着使用动态规划查找最大概率路径。

除了基于词典分词，jieba还是用HMM处理未记录词汇或切分不明确。HMM作为统计模型，考虑词语之间概率关系和状态转移概率。

## HMM

HMM基于马尔可夫过程和概率图模型的概念，是一种生成模型，用于描述两个相关序列的依赖关系，即状态序列和观测序列。它可以根据观测序列来推测隐藏的状态序列。

### HMM的基本要素

- 状态转移概率：某个时刻，状态转移到另一个状态的转移概率；
- 观测概率：某个状态下，生成某个观测符号的概率；
- 初始状态概率：在第一个时刻，各个状态的概率分布。

### HMM的基本假设

- 马尔科夫性，即下一个状态只与当前状态有关，与其他状态无关；
- 齐次性，即不同时刻的状态转移概率和观测概率都相同；
- 观测独立性，即某个时刻的观测值只依赖于该时刻的状态值，与其他时刻的观测值和状态值无关。

### 基本运作原理

将整个序列看作是一系列的状态和观测符号的组合，根据这些状态和观测符号的转移概率和生成概率，对序列进行建模和分析。

**基本思路**：将状态序列和观测序列分开处理，然后根据概率分布将它们组合在一起。通过建立模型并应用模型，可以计算出各种感兴趣的事件的概率，从而解决我们感兴趣的问题。

### Viterbi算法

Viterbi算法是一种动态规划算法，用于寻找最有可能产生观测事件序列的隐含状态序列。它被广泛应用于解决概率图模型中的最优化问题

核心思想是通过动态规划求解最优路径。具体来说，它利用动态规划的思想，从观测序列的最后一个观测点开始，逐步向前计算每个观测点对应的最优状态序列。在这个过程中，Viterbi算法不断根据当前观测状态和模型参数计算出每个状态转移到下一个状态的概率，并选择概率最大的路径作为当前观测点的最优路径。

1. 初始化局部状态
2. 动态规划时刻$t$局部状态
3. 计算$T$时刻最有可能隐藏状态序列出现概率
4. 利用局部状态回溯

## Word2Vec 原理

Word2Vec 是将自然语言生成词向量的模型，主要是将每个词汇映射为向量，且使得语义相近的词汇在向量空间距离较近。

Word2Vec有两种模型：

- CBOW词袋模型：根据上下文词汇预测目标词汇，将上下文词汇的词向量进行计算，然后通过神经网络预测目标词汇，适用训练数据较大情况。
- Skip-gram模型：根据目标词汇预测上下文词汇，通常训练数据较少比较好，因为可以更好处理罕见词汇。

### Word2Vec 训练过程

1. 构建包含语料库不同词汇的词汇表

2. 对每个词汇，初始化一个随机词向量

3. 选择固定大小上下文窗口

4. 对于每个词汇生成训练样本，并使用梯度下降等优化算法训练神经网络

   **损失函数**为对数似然函数

### 优化策略

- **层次softmax（Hierarchical Softmax）**：时间复杂度从V降到logV （V为词表的大小）

- **负采样**：时间复杂度从V下降到NnegN_{neg}Nneg+1（NnegN_{neg}Nneg为负采样的样本个数）

  对优化目标采用负采样方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担

- 二次采样subsampling：对高频次单词进行抽样来减少训练样本的个数

- 将常见的单词组合或者词组作为单个“words”来处理

- **FastText**

  使用word2vec的结构训练，加上了ngram，ngram一方面可以增加信息，一方面避免OOV。

### glove word2vec 区别

GloVe和Word2Vec都是强大的词向量模型。

- 训练方法：

  GloVe是一种基于全局语料库统计信息的模型。它使用全局共现矩阵来学习单词的分布式表示。GloVe使用矩阵分解技术，将共现矩阵分解为两个低秩矩阵的乘积，其中一个矩阵表示单词的词向量，另一个矩阵表示单词之间的共现关系。

  Word2Vec是一种基于神经网络的模型，使用前馈神经网络进行训练。

- 训练效率：

  GloVe：由于其基于全局统计信息的方法，通常需要更多的训练时间和计算资源，尤其在大规模语料库上。

  Word2Vec：相对于GloVe，Word2Vec通常更快速地训练，因为它采用了神经网络的前馈训练方法，并且可以使用负采样（Negative Sampling）来加速训练。

- 词向量性能：

  GloVe：GloVe的词向量在一些语义任务上表现得很好，可以捕捉全局共现关系，因此在一些相似度和关联性任务中表现出色。

  Word2Vec的词向量在大多数情况下也表现得很好，但可能更适用于一些局部上下文任务，如语法和句法任务。

## GPT 原理

可迁移到多种NLP任务的，基于Transformer的语言模型。 模型的训练分两步：

1. 无监督的预训练 目标函数是最大化似然函数。
2. 有监督的训练数据

### GPT优缺点

优点：

1. 循环神经网所捕获到的信息较少，而transformer可以捕获到**更长范围的信息**。
2. 计算速度比循环神经网络更快，易于**并行化**
3. 实验结果显示，Transformer的**效果**比ELMo和LSTM网络**更好**

缺点： 对于某些类型的任务**需要对输入数据的结构作调整**。

### Bert和GPT的区别

- **目标任务**：BERT是一种双向的预训练语言模型，可以用于多种自然语言处理任务，而GPT则是一种单向的预训练语言模型，主要用于生成式任务

- **预训练方式**：BERT采用了Masked Language Model和Next Sentence Prediction两个任务来进行预训练。在Masked Language Model任务中，随机遮挡输入序列中的15%的token，然后根据上下文推测这些被替换的token。在Next Sentence Prediction任务中，判断两个句子是否相邻。而GPT则只采用了单一的Language Modeling任务，根据上下文推测被替换的token。

- **模型结构**：BERT的网络结构类似于Transformer的Encoder部分，而GPT类似于Transformer的Decoder部分。BERT的输入序列会被分割成若干个token，然后通过网络进行编码，得到每个token的向量表示。而GPT则是根据上文进行解码，生成下一个token。

## Transformer原理

Transformer是一种用于自然语言处理的深度学习模型，利用Attention机制。它的架构是Encoder-Decoder模型。

在编码器中，输入序列经过词嵌入和位置嵌入后，通过多层的编码器层进行叠加。每层编码器由多个相同的子层组成，每个子层都包含一个多头自注意力机制和一个前馈神经网络。多头自注意力机制通过对每个词进行多次独立的注意力计算，生成多个头部的注意力向量，然后通过对这些注意力向量进行叠加，得到最终的输出向量。前馈神经网络则对每个词的前后词关系进行建模，进一步增强词之间的联系。

解码器也由多个相同的层叠加而成，每层解码器也包含一个多头自注意力机制和一个前馈神经网络。不过，在解码器中，自注意力机制的位置有所不同，它同时考虑了当前词的前后词和已经生成的输出序列中的词。这样，解码器可以更好地根据上下文生成新的词，提高了生成序列的准确性和流畅性。

除了编码器和解码器，Transformer还包括一个预训练阶段，用于在大量的无监督语料库上进行预训练，从而使得模型更好地理解自然语言文本。预训练阶段通常采用Masked Language Model和Next Sentence Prediction两个任务。

### 公式

$$
Att(Q,K,V)=\text{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V
$$

- Q、K、V是输入的查询（Query）、键值（Key）、值（Value）向量
- $d_k=\dfrac{d_{model}}k$，embedding 的 dimension
- softmax表示对权重进行归一化

### attention矩阵的计算过程中要除以根号dk的原因

1. 防止输入softmax的值过大，导致偏导数趋近于0，造成梯度消失的问题。
2. 选择根号dk是因为可以使得q*k的结果满足期望为0，方差为1的分布，类似于归一化。
3. 可以确保不管输入的维度如何变化，注意力分数的相对权重保持一致。

### Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？

decoder自注意力需要进行sequence mask。

让输入序列只看到过去的信息，而看不到未来的信息。

### Transformer的并行化体现在哪里，Decoder可以做并行化嘛？

Encoder的模块是串行的，但模块内的子模块多头注意力和前馈网络内部都是并行的，因为单词之间没有依赖关系。

Decode引入sequence mask就是为了并行化训练，推理过程不并行

### Encoder端和Decoder端是如何进行交互的？

Cross Self-attention，**Decoder提供**Q，Encoder提供K，V。

### Transformer中为什么需要线性变换？

在$QK^T$部分，线性变换矩阵将KQ投影到不同空间，增加表达能力，计算得到的注意力矩阵的**泛化能力更高**

### Transformer attention的注意力矩阵的计算为什么用乘法而不是加法？

为了计算更快。在计算复杂度上，乘法和加法理论上的复杂度相似，但是在实践中，乘法可以利用高度优化的矩阵乘法代码

### Softmax(Q*V^T)*V跟Softmax(Q*K^T)*V相比有什么区别

$QV^T$计算简单，但是在处理长序列时，计算量会非常大，而且容易出现梯度消失或爆炸的问题。

$QK^T$增加了对key的信息利用，可以更好地捕捉文本中的语义信息。在处理长序列时，计算效率更高，也更加稳定。

### Transformer attention计算注意力矩阵的时候如何对padding做mask操作的？

padding位置置为-1000，再对注意力矩阵进行相加。

### Transformer的残差结构及意义

解决梯度消失，防止过拟合

### transformer有什么可以调整的参数

- 学习率：模型训练过程中的更新速度。较大可能导致模型训练不稳定，较小可能导致训练速度较慢。

- 批量大小（Batch Size）：批量大小决定了模型在一次更新中处理的样本数量。

- 层数（Number of Layers）：Transformer模型中有多个编码器和解码器层，增加层数可以提高模型的深度和表达能力。但是，过深的层数可能会导致过拟合问题。可以通过实验来找到最佳的层数。

- d_model：表示embedding的维度，通常设置为512或者1024。这个参数会影响模型的表达能力和训练速度。

- d_ff：表示feed-forward层的宽度，通常设置为2048或4096。这个参数也会影响模型的训练速度和表达能力。

- num_heads：表示多头自注意力机制的头数，通常设置为8或16。这个参数会影响模型的并行计算能力和表达能力。

- dropout概率：用于防止过拟合的技术，通常设置为0.1或0.2。

### Transformer 优缺点

**优点**：

- 可并行
- 独立于卷积和循环，完全依赖于attention处理全局依赖，解决长距离依赖问题
- 性能强

**缺点**：

- 长度固定
- **局部信息的获取**不如RNN和CNN强：Transformer关注的全局关系，而RNN在计算过程中更关注局部，对距离更加敏感。

### transformer相比于CNN存在的问题有哪些

- Transformer模型通常比CNN更加庞大，需要更多计算数字和计算资源
- Transformer最初设计用于自然语言处理，它在文本数据上表现出色。但对于其他类型的数据，特别是图像数据，CNN通常更适用。
- 相对于传统的CNN结构，Transformer的内部结构较复杂，因此对于模型的解释性较差。

### Transformer、LSTM和单纯的前馈神经网络比，有哪些提升？

LSTM相比于单纯的前馈神经网络，首先具有理解文本的**语序关系**的能力（RNN）。除此之外，又解决了RNN在处理长序列时发生的梯度消失和梯度爆炸的问题。

Transformer进一步解决了RNN、LSTM等模型的**长距离依赖**问题，能够理解更长的上下文语义。可以**并行化**，所要的训练时间更短。

## 简述 Bert 模型

BERT 是一种是一种深度学习的NLP模型，能够在大规模文本数据上进行预训练，然后在特定任务上进行微调，以获得出色的性能。

### Transformer

BERT使用了Transformer模型的架构，包括多头自注意力机制和前馈神经网络。这种架构使模型能够有效地捕捉长距离依赖关系。

BERT使用了Transformer编码器的堆叠，通常是12层或24层，用于编码输入文本的信息。这些编码器可以在不同任务上进行微调，因此BERT是一个通用的预训练模型，适用于各种NLP任务。

### 预训练

BERT在预训练过程中执行了两个任务：

- 掩盖语言模型MLM：要求模型预测在输入句子中随机掩盖掉的单词
- 下一句预测NSP：NSP任务要求模型判断两个句子是否是原始文本中相邻的

### 优化器

Adam

### 常用的mask方法及特点

mask策略的好处：

）一定程度上缓解了初始做法导致的预训练模型和微调模型不一致的问题，减少了[MASK]的数量；

mask策略的缺点：

10%保留为原来的词，10%替换为该句子的其他部分，使得模型并不知道输入对应位置的词汇是否为正确的词汇，这迫使模型更多地依赖上下文预测词汇，并赋予了模型纠错的能力。

### RoBERTa在Bert基础进行哪些改进

1. 动态掩码：RoBERTa将预训练的数据复制10份，每一份都随机选择15%的Tokens进行mask，然后每份数据都训练N/10个epoch。这相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。而BERT是静态的，也就是说从一开始选择了15%的Tokens进行mask后，接下来的N个epoch里，这些被mask的tokens就不会再改变。
2. 去掉NSP任务：RoBERTa去掉了NSP任务，使用FULL-SENTENCES训练方式。每次输入连续的多个句子，直到最大长度512。而BERT使用了NSP任务。
3. 使用更大的mini-batch和更多的数据：RoBERTa使用了更大的mini-batch和更多的数据，实验效果更好。

### Bert 分词

BERT使用了一种特殊的分词方法，称为WordPiece分词。

WordPiece分词器将输入的文本标记化成更小的单元，这些单元可以是整个单词、子词或字符。WordPiece分词是一种基于最大匹配的分词方法，它从文本的开头开始，逐渐构建一个最长的子词或单词，然后移动到下一个未处理的部分，重复这个过程。这样做的好处是能够捕捉到单词的内部结构，同时还能够处理不常见的词汇和新词。

### 残差连接的作用

防止梯度消失

### 激活函数用了啥

gelu

**relu和gelu区别**

relu=max(0,x)。计算简单，训练时可以加速收敛

GeLU是一种基于高斯误差函数和tanh函数的组合，在负数输入时不会完全变成零，而是将其平滑地映射到一个小的非零值，这有助于减轻了ReLU中的“死亡神经元”问题。但计算略微复杂

### BERT缺点

- 从MLM预训练任务分析

- 其中**1、2**两点都基于自编码的痛点。

  1. 预训练阶段会出现特殊的[MASK]字符，而在下游任务中不会出现，造成**预训练和微调之间的不匹配**。

     解决方案：

     - XLNet：使用**自回归**代替自编码。自回归是指使用前向语言模型或后向语言模型来预测当前位置的词，有单向的缺点；自编码是BERT所使用的MLM的训练方法，其能使用到双向信息但是会造成预训练与微调时的不一致。

  2. 基于**不符合真实情况的假设：被mask掉的token是相互独立的**。如“自然语言处理”，如果mask掉“语言”，显然“语“和”言“并不是相互独立的，即”语“后面预测”言“的概率要高于预测为其他字的概率。

     解决方案：

     - XLNet：使用**自回归**代替自编码。

  3. BERT中词被切分为更细粒度的Token，在随机Mask的过程中会出现**只Mask掉一个词的一部分的情况**，这样会使得预测任务变得更简单，因为通过一个词的前后部分很容易预测出当前词。

     解决方案：

     - BERT-wwn：对全词mask，对于中文来说就是mask词语而不是字，对于英文来说就是mask单词而不是token
     - ERNIE：百度的。基于实体和短语的mask

  4. 每个batch只有15%的token会被预测，所有**收敛速度会比传统语言模型慢**，因为传统语言模型会预测每个token

  5. **静态掩码**。BERT在数据预处理期间执行一次掩码，得到静态掩码。

  1. 解决方案：
     - RoBERTa：动态掩码。在每次模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。

  从NSP预训练任务分析

  1. NSP任务的有效性被质疑，可能是由于NSP任务相比于MLM过于简单。

     解决方案：

     - RoBERTa：删除NSP预训练任务
     - ALBERT：使用SOP任务代替NSP任务，即预测两个句子顺序是否正确。

  模型结构分析

  1. 词向量维度和隐层维度相同，参数量大。
     1. ALBERT：在词表和隐层之间增加一个维度E（应该就是模型内部的计算维度吧）；
  2. 缺乏生成能力
  3. 只能处理两个segment
     - XLNet：Relative Segment Encoding。只判断两个token是否属于同一个segment，而不判断他们各自属于哪个segment。具体实现是在计算attention weight的时候，给query额外算出一个权重加到原来的权重上，类似于relative positional encoding。
  4. 长度限定512
     - XLNet：沿用了Transformer-XL的思想，可处理更长文本。

  

## 评价指标是怎样设定的

评价指标的设定取决于所解决的具体任务。不同的NLP任务可能需要不同的评价指标，以便更好地度量模型性能。

比如命名实体识别常用

- **准确度**：正确标识的实体比例。
- **F1分数**：通常也用于NER任务，综合考虑精确度和召回率。
- **命名实体级别的评价指标**：例如，每个命名实体类型的精确度、召回率和F1分数。

问答系统：

- **精确匹配率**：模型生成的答案是否与标准答案完全匹配。
- **F1分数**：用于评估答案的部分匹配性能，通常计算答案与标准答案的重叠程度。
- **BLEU分数**：在机器翻译和生成式问答中，用于评估生成文本与标准答案之间的相似度。

# Pytorch

## Pytorch如何加载数据

在Pytorch中，可以使用数据加载器（Data Loader）来加载数据。数据加载器能够将数据集划分为不同的批次，并对数据进行随机打乱和洗牌，以便在训练深度学习模型时使用。

1. 定义一个继承自`torch.utils.data.Dataset`的自定义数据集类
2. 将自定义数据集类传递给`torch.utils.data.DataLoader`构造函数，同时指定批次大小（batch size）以及其他选项。
3. 使用`DataLoader`对象的`__iter__()`方法迭代数据批次，并在模型训练过程中使用这些批次。

## train,val模块

在PyTorch中，通常需要自定义训练（train）和验证（validation）循环来训练和评估深度学习模型。

**train**

- 加载数据集：使用PyTorch提供的DataLoader加载训练集
- 定义模型：根据任务需求，定义一个深度学习模型，并设置模型的参数。
- 定义损失函数：根据任务类型，选择适合的损失函数，例如交叉熵损失或均方误差损失等。
- 优化器：选择适合的优化器，例如Adam或SGD等，用于更新模型的参数。
- 训练模型：在训练过程中，通过迭代数据集，计算损失并进行反向传播，更新模型的参数。

**val**

- 加载数据集：使用PyTorch提供的DataLoader加载验证集。
- 模型评估：将训练好的模型在验证集上进行评估，计算评估指标
- 模型调整：根据评估结果，对模型进行调整和优化，以提高模型的性能。

## Pytorch分布式训练的原理

PyTorch的分布式训练是指将模型放置在多台机器上，并在每台机器上的多个GPU上进行训练。这种训练方式主要用于处理大规模数据和模型，以及加快训练速度。

1. 数据并行化（Data Parallel）：这种做法主要是依照一些规则将数据分配到不同的GPU上，每个GPU都有相同的模型构架，各自进行训练后，将计算结果合并，再进行参数更新。
2. 模型平行化（Model Parallel）：当模型的规模非常大，无法在单个GPU上运行时，可以采用模型平行化的方式。这种方式是将模型的不同部分分别放到不同的GPU上进行处理，每个GPU只负责模型的一部分。这样就可以同时利用多个GPU的计算能力，加速模型的训练。

# 算法

## TopK

## 编辑距离

## 原地归并两个有序数组
