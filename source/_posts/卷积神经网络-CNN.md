---
title: 卷积神经网络 CNN
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - AI
tags:
  - AI
  - CNN
keywords: 卷积神经网络 CNN
abbrlink: 卷积神经网络 CNN
date: 2023-08-03 08:59:54
---

卷积神经网络是一种专门用来处理具有类似网络结构的神剧的神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作二维的像素网格）。

<!--more-->

# LeNet 架构

LeNet是推进深度学习领域发展的最早的卷积神经网络之一。当时，LeNet架构主要用于字符识别任务，比如读取邮政编码、数字等等。

![LeNet 架构](https://raw.githubusercontent.com/Jarrycow/picHost/main/JavaWeb/LeNet%E6%9E%B6%E6%9E%84.webp)

上述主要有四个操作：

1. 卷积

2. 非线性处理（ReLU）

3. 池化或者亚采样

4. 分类（全连接层）

## 卷积

卷积神经网络的名字就来自于其中的卷积操作。卷积的主要目的是为了从输入图像中提取特征。卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。

![卷积](https://raw.githubusercontent.com/Jarrycow/picHost/main/JavaWeb/%E5%8D%B7%E7%A7%AF.webp)

用$\lceil$核$\rfloor$<sub>橙色矩阵</sub>在原始矩阵上滑动，每次滑动一个$\lceil$步长$\rfloor$，在每个位置上，我们计算对应元素的乘积（两个矩阵间），并把乘积的和作为最后的结果，得到输出矩阵$\lceil$特征图$\rfloor$中的每一个元素的值。

**CNN会在训练过程中学习到这些核的值**（尽管如此，依然需要在训练前指定诸如核的个数、核的大小、网络架构等参数）。我们使用的核越多，提取到的图像特征就越多，网络所能在未知图像上识别的模式也就越好。

特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们：

- **深度：**深度对应的是卷积操作所需的滤波器个数。比如若使用三个不同的滤波器对原始图像进行卷积操作，这样就可以生成三个不同的特征图。可以把这三个特征图看作是堆叠的2d矩阵，那么，特征图的“深度”就是三。
- **步长：**步长是我们在输入矩阵上滑动滤波矩阵的像素数。当步长为1时，我们每次移动滤波器一个像素的位置。当步长为2时，我们每次移动滤波器会跳过2个像素。步长越大，将会得到更小的特征图。
- **零填充：**有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。零填充的一大好处是可以让我们控制特征图的大小。使用零填充的也叫做泛卷积，不适用零填充的叫做严格卷积。

## 激活函数 ReLu

ReLU是一个元素级别的操作（应用到各个像素），并将特征图中的所有小于0的像素值设置为零。

ReLU的目的是在ConvNet中引入非线性，因为在大部分的我们希望 ConvNet学习的实际数据是非线性的；而卷积操作只应用到了相乘相加，是线性操作。

## 池化

空间池化降低了各个特征图的维度，但可以保持大部分重要的信息。空间池化有下面几种方式：最大化、平均化、加和等等。在实际中，最大池化被证明效果更好一些。

![池化](https://raw.githubusercontent.com/Jarrycow/picHost/main/JavaWeb/%E6%B1%A0%E5%8C%96.png)

池化函数可以逐渐降低输入表示的空间尺度

- 使输入特征维度更小，网络中的参数和计算的数量更加可控的减小，因此可以控制过拟合
- 使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化/平均值的操作
- 获取图像最大程度上的尺度不变性。可以检测图像中的物体，无论它们位置在哪里

## 全连接层

全连接层是传统的多层感知器，在输出层使用的是softmax激活函数<sub>也可以使用其他像SVM的分类器</sub>。**卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类**。

除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。

从全连接层得到的输出概率和为1。这个可以在输出层使用softmax作为激活函数进行保证。softmax函数输入一个任意大于0值的矢量，并把它们转换为零一之间的数值矢量，其和为一。

# 训练过程

1. 初始化所有的滤波器，使用随机值设置参数/权重
2. 网络接收一张训练图像作为输入，通过前向传播<sub>卷积、ReLU、池化，以及全连接层的前向传播</sub>，找到各个类的输出概率
3. 在输出层计算总误差
4. 使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值/权重以及参数的值，使输出误差最小化
5. 对训练数据中所有的图像重复步骤1~4

当一张新的（未见过的）图像作为ConvNet的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。

# 参考资料

[芦苇的机器学习笔记 CNN卷积神经网络](https://luweikxy.gitbook.io/machine-learning-notes/convolutional-neural-network#shi-ji-xun-lian-wen-ti)