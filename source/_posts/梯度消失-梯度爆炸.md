---
title: 梯度消失 & 梯度爆炸
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - AI
tags:
  - AI
  - 机器学习
keywords: 梯度消失 & 梯度爆炸
abbrlink: 梯度消失 & 梯度爆炸
date: 2023-07-17 23:02:44
---

梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。

<!--more-->

# 梯度消失 & 梯度爆炸出现的原因

对于下图所示的含有 3 个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的 hidden layer 3 等的权值更新相对正常，但前面的 hidden layer 1 的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值。即此深层网络的学习就等价于只有后几层的浅层网络的学习了。

![](https://pic2.zhimg.com/v2-82873a89ff3c14c1d3b42d1862917f35_r.jpg)

以下图反向传播 $y=\sigma \left( z_i \right) =\sigma \left( w_ix_i+b_i \right) $ 为例

![](https://pic3.zhimg.com/v2-b9e0d6871fbcae05d602bab65620a3ca_r.jpg)

则 $\dfrac{\partial C}{\partial b_1}=\dfrac{\partial C}{\partial y_4}\sigma'\left(z_4\right)w_4\cdot \sigma'(z_3)w_3\cdot \sigma'(z_2)w_2\cdot\sigma'(z_1)$。但 $sigmod$ 导数图如下，最大也只能是 0.25。

![](https://pic2.zhimg.com/80/v2-da5606a2eebd4d9b6ac4095b398dacf5_1440w.webp)

因此对于上面的链式求导，层数越多，求导结果越小，因而导致梯度消失的情况出现。同理，梯度爆炸的原因就是 $\left|\sigma'(z)w\right|>1$，即 $w$ 比较大。

其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。

# 解决方案

## 预训练

每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”。预训练完成后，再对整个网络进行“微调”。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

## 梯度剪切 正则

**梯度剪切**这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

**正则化**是通过对网络权重做正则限制过拟合。损失函数表达形式：
$$
LOSS=\left(y-W^Tx\right)^2+\alpha\lVert W \rVert ^2
$$
其中，$\alpha$ 为正则项系数。因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。但实际操作中梯度消失出现的更多一些

## 激活函数

**tanh 激活函数**

$$
\tanh (x)=\dfrac{\text e^x-\text e^{-x}}{\text e^x+\text e^{-x}}
$$

tanh激活函数不能有效的改善这个问题——虽然比sigmoid的好一点，sigmoid的最大值小于0.25，tanh的最大值小于1，但仍是小于1的。

![tanh 函数及其导数](https://pic4.zhimg.com/v2-66a7e4fcf11a2d85c15e7bf7b88b2d1b_r.jpg)

**Relu 激活函数**
$$
Relu(x)=\max(x,0)
$$


Relu 函数思想是，只要激活函数导数为1，那么就不存在梯度消失爆炸了，每层网络以相同速度更新。

![Relu 函数及其导数](https://pic2.zhimg.com/v2-55475ee2d90cd7257a39f62549a65769_r.jpg)

优点：

- 解决了梯度消失、爆炸的问题
- 计算方便，计算速度快
- 加速了网络的训练

缺点：

- 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
- 输出不是以0为中心的

**leakrelu**

leakrelu就是为了解决relu的0区间带来的影响。
$$
leakrelu=f\left( x \right) =\left\{ \begin{array}{l}
	x,\ x>0\\
	kx,\ x<0\\
\end{array} \right. 
$$
其中k是leak系数，一般选择0.1或者0.2。

**elu**

elu激活函数也是为了解决relu的0区间带来的影响。
$$
elu=f\left( x \right) =\left\{ \begin{array}{l}
	x,\ x>0\\
	\alpha \left( \text{e}^x-1 \right) ,\ x<0\\
\end{array} \right. 
$$


![img](https://pic3.zhimg.com/v2-ec3c80e51129bd76d49cad6e52d449c2_r.jpg)

但是elu相对于leakrelu来说，计算要更耗时间一些

## 批规范化 BN

Batchnorm 目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果。即通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。

具体即反向传播中，每一层的梯度会乘以该层的权重。一般正向传播中 $f_3=f_2\left(w^Tx+b\right)$，则反向传播中 $\dfrac{\partial f_2}{\partial x}=\dfrac{\partial f_2}{\partial f_1}w$。反向传播中有 $w$ 存在，所以 $w $ 大小影响梯度的消失和爆炸。

BN 通过对每一层的输出做scale和shift的方法，通过一定的规范化手段，**把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布**。

## 残差结构

输入加到某层中，求导时总有个1，就不会梯度消失了
$$
\dfrac{\partial \text{loss}}{\partial x_1}=\dfrac{\partial\text{loss}}{\partial x_L}\cdot \dfrac{\partial x_L}{\partial x_l}=\dfrac{\partial \text{loss}}{\partial x_L}\cdot\left(1+\dfrac{\partial}{\partial x_L}\sum_{i=l}^{L-1}F\left(x_i,W_i\right)\right)
$$


![残差结构](https://pic4.zhimg.com/v2-3134d24348c47ca2001d37fef1c3f8bf_r.jpg)

## LSTM

RNN和DNN的梯度消失问题略有不同。DNN中各个权重的梯度是独立的，该消失的就会消失，不会消失的就不会消失；RNN权重共享，每一时刻都由前面所有时刻共同决定，是一个相加的过程，但距离长了，计算最前面的导数时，最前面的导数就会消失或爆炸，但当前时刻整体的梯度并不会消失。一般 RNN 梯度消失指的是当下梯度用不到前面的梯度了。

LSTM 传播有多条路径，$c_{t-1}\rightarrow c_t=f_t\odot c_{t-1}+i_t\odot \hat{c}_t$。这条路径上只有逐元素相乘和相加的操作，梯度流最稳。即这条路上没有遗忘门，梯度畅通无阻，不会消失，类似于残差链接。



# 参考文献

[神经网络训练中的梯度消失与梯度爆炸](https://zhuanlan.zhihu.com/p/25631496)

[从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导）](https://zhuanlan.zhihu.com/p/76772734)