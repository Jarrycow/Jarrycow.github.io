---
title: 正则化
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - AI
tags:
  - 机器学习
  - AI
keywords: 正则化
abbrlink: 正则化
date: 2023-07-17 22:15:25
---

正则化是为了防止过拟合，在原来的 loss function 基础上，加上了一些正则化项或者称为模型复杂度惩罚项。

<!--more-->

# 为什么要正则化

举例来说，目前优化函数为 $f\left(x_i\right)=w_0x_0+w_1x_1+\cdots+w_nx_n$

如何防止过拟合？显而易见，应当尽量避免过拟合出现的特征。而过拟合时，出现的特征项比较多，$w_0,w_1,\cdots w_n$ 等量增长。那么防止过拟合，就应该让 $w_0,w_1,\cdots w_n$ 个数最小化，即 0 范数。

因此，除了要使 $LOSS$ 最小，还要让 $r\left(d\right)=\left|W\right|_0$ 最小。为了同时满足两者最小化，可以求解让 $LOSS+r\left(d\right)$ 最小。

# L1 & L2

但是经过各位野兽先辈的努力，发现 0 范数很恶心，求解是 NP 完全问题，因此采取 1 范数和 2 2 范数。

loss function 后加上 L1 正则化容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。

# 参考文献

[机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039/answer/131421690)

[机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039/answer/45397882)