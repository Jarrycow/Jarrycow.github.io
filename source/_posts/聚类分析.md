---
title: 聚类分析
author: Jarrycow
img: /medias/featureimages/xxx.png
cover: false
top: false
mathjax: true
categories:
  - 模式识别
tags:
  - 机器学习
keywords: 聚类分析
abbrlink: Cluster_Analysis
date: 2022-10-14 15:20:12
---

物以类聚，人以群分

<!--more-->

## 距离

以模式样本特征向量之间的距离，来作为样本相似度的依据

### 欧氏距离

$D(X_1X_2)=\left|X_1-X_2\right|=\sqrt{(X_1-X_2)^T(X_1-X_2)}\\=\sqrt{(x_{11}-x_{21})^2+\cdots +(x_{1n}-x_{2n})^2}$

**<font color="red">注：要对数据标准化，以防单位不同导致量纲不同</font>**

**代码**

```python
def EuclidDistance(data1, data2):  
    '''
    函数：计算欧拉距离
    参数：
      - data1：数据1([Number])
      - data2：数据2([Number])
    返回值：
      - 距离(Number)
    '''
    distance = 0
    for i in range(len(data1)):
        distance += pow((data1[i]-data2[i]), 2)
    distance = math.sqrt(distance)
    return distance
```

### 马氏距离

样本点到均值向量的距离

聚类分析中经常使用的一个判定准则是样本的到均值向量的距离是否小于给定的阈值

$X$——模式向量

$M$——均值向量

$C$——协方差矩阵
$$
D(X)=\sqrt{\left(X-M\right)^TC^{-1}\left(X-M\right)}\\
D(X_i,X_j)
$$
其中

$C=E\{(X-M)(X-M)^T\}
=E \left\{ \begin{bmatrix}
 x_1-m_1 \\ x_2-m_2 \\ \vdots  \\ x_n-m_n
\end{bmatrix}\begin{bmatrix}
 (x_1-m_1) & (x_2-m_2) & \cdots  & (x_n-m_n)
\end{bmatrix} \right\}\\==\begin{bmatrix}
E(x_1-m_1)(x_1-m_1) & E(x_1-m_1)(x_2-m_2) & \cdots & E(x_1-m_1)(x_n-m_n)\\
E(x_2-m_2)(x_1-m_1) & E(x_2-m_2)(x_2-m_2) & \cdots & E(x_2-m_2)(x_n-m_n)\\
\vdots & \vdots & \vdots & \vdots\\
E(x_n-m_n)(x_1-m_1) & E(x_n-m_n)(x_2-m_2) & \cdots & E(x_n-m_n)(x_n-m_n)
\end{bmatrix}
\\= \begin{bmatrix}
\sigma^2_{11} & \sigma^2_{12} & \cdots & \sigma^2_{1n}\\
\sigma^2_{21} & \ddots & \sigma^2_{jk} & \vdots \\
\vdots & \vdots & \sigma_{kk}^2 \vdots\\
\sigma^2_{n1} &\cdots & \cdots & \sigma^2_{nn}
\end{bmatrix}$

![](聚类分析\欧氏和马氏.png)

马氏距离的计算是建立在总体样本的基础上的。即相同的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的

虽然标准化欧氏距离的数据可以消除维度差异，但是若维度间不独立同分布，样本之间并非越近概率越大。

比如样本服从$f(x) = x$的分布,A与B相对于原点的距离依旧相等，显然A更像是一个离群点。因此要对其主成分分析中的主成分进行标准化，将变量按照主成分进行旋转，让维度相互独立并标准化。

![](https://pic3.zhimg.com/80/v2-3cee35b79d272dda86e2604c160934ee_720w.webp)
**代码**

```python
def MaharanobisDistance(data, data1, data2 = None):  
    '''
    函数：计算马氏距离
    参数：
      - data1：数据1([Number])
      - data2：数据2([Number])
      - data：总体数据([m*n])
    返回值：
      - 距离(Number)
    '''
    distance = 0
    data = np.array(data)  # 转换为array格式
    data1 = np.array(data1)

    C = np.cov(data.T)  # 协方差矩阵
    inC = np.linalg.inv(C)  # 协方差逆矩阵
    if(data2 == None):  # 求一个样本到中心的距离
        data2 = data.mean(axis=0)   # 样本均值
    distance = math.sqrt(np.dot(np.dot(data1 - data2, inC), (data1 - data2).T))  # 根号{(x-y)^T·S^{-1}·(x-y)}
    return distance
```

**缺点**

- 协方差矩阵必须满秩，即数据个数必须大于等于特征个数
- 只对线性空间有效，对于流线型只能局部定义

### 明氏距离

多维连续空间的距离

$n$维模式样本向量$X_i$、$X_j$之间明氏距离
$$
D_m(X_i,X_j)=\left[\sum_{k=1}^n \left|x_{ik}-x_{jk}\right|^m\right]^\frac1m
$$

**缺点：**

- $m=2$为欧氏距离
- $m=1$为曼哈顿距离
- $m\rightarrow \infty$为切比雪夫距离

<img src="https://pic3.zhimg.com/80/v2-d988aba5a9490f66b1c8e91c84d7b102_720w.webp" style="zoom:67%;" />



### 汉明距离

设$X_i,X_j$为$n$维二值($1$或$-1$)模式样本向量
$$
D_h(X_i,X_j)=\dfrac12\left(n-\sum^n_{k=1}x_{ik}\cdot x_{jk}\right)
$$
其中$x_{ik}$、$x_{jk}$分别表示$X_i$和$X_j$的第$k$个分量

### 角度相似性函数

$S(X_i,X_j)=\dfrac{X_i^TX_j}{||X_i||\cdot||X_j||}$

为模式向量$X_i$和$X_j$之间夹角的余弦

## 基于距离阈值的聚类算法

### 近邻聚类法

1. 任取样本$X_i$作为第一个聚类中心的初始值，令$Z_1=X_1$

2. 计算样本$X_2$到$Z_1$的欧氏距离$D_{21}=\left|\left|X_2-Z_1\right|\right|$

   若$D_{21}>T$，定义新聚类中心$Z_2=X_2$

   否则$X_2\in Z_1$为中心的聚类

3. 若已有聚类中心$Z_1、Z_2$，计算$D_{31}、D_{32}$

   若$D_{31}>T$切$D_{32}>T$，则建立第三个聚类中心$Z_3=X_3$

   否则$X_3\in \min\left\{Z_1,Z_3\right\}$为中心的聚类

4. 依此类推，直到将所有的$N$个样本都进行分类

**算法特点**

- **局限性：**很大程度上依赖于距离阈值$T$的大小、第一个聚类中心的位置选择、待分类模式样本的排列次序、以及样本分布的几何性质等
- **优点：**计算简单
